## 6. 순환 신경망(RNN)
### 6.1 순환 신경망의 기초
- 시계열 데이터 : 시간 흐름에 따라 순차적으로 수집된 데이터 -> 관측값이 시간적 의존성을 가짐
- RNN은 기본적으로 과거의 정보를 기억하면서 새로운 정보를 처리
- CNN이 이미지 데이터의 공간적 특징을 추출하여 학습한다면 RNN은 시계열 데이터의 시간적 특징을 추출하여 학습
- Feed Forward Neural Network : 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향함
- RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 지님
- 은닉 상태 : 순환 신경망의 핵심으로 네트워크 메모리 역할을 함

$$
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b)
$$

  - $W_x$: 입력 가중치  
  - $W_h$: 이전 은닉 상태 가중치  
  - $b$: 편향 (bias)
- 순환 신경망의 순전파와 역전파
  - 순전파 : 이전에 얻은 기억을 다음으로 넘김 ex) 나는 -> 밥을 -> 먹었다.
  - 역전파 : 틀린 이유를 과거까지 거슬러 올라가서 찾는 과정 = 출력 → 시간을 거슬러 → 가중치 수정
  - 과거로 갈수록 신호가 점점 약해짐 -> 기울기가 점점 0으로 -> 오래된 정보가 거의 학습이 안 됨
  - 반면에 가중치가 1보다 큰 경우에는 가중치 폭발이 일어남
  - `clip_grad_norm_`을 통해 이미 계산된 기울기를 잘라낼 수 있음
  ### 6.2 고급 순환 신경망 아키텍처
1. LSTM : 전통적인 RNN의 단점을 보완한 장단기 메모리
  - 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함
  - 입력 게이트 : 현재 정보를 기억하기 위한 게이트
  - 망각 게이트 : 셀 상태에서 어떤 정보를 버릴지 결정
  - 셀 상태 : 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함 -> 이 값을 현재 시점 t의 셀 상태라고 하며, 이 값은 다음 t+1 시점의 LSTM 셀로 넘겨지게 됨
  - 출력 게이트 : 셀 상태에서 어떤 정보를 출력할지 결정 -> 출력 게이트는 현재 시점 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값
  - 셀 상태가 하이퍼볼릭탄젠트 함수를 지나 -1~1 사이의 값이 되고 해당 값은 출력 게이트의 값과 연산되며 값이 걸러짐 -> 은닉 상태
2. GRU : LSTM의 간소화 버전 -> 장기 의존성 학습 능력을 유지하면서도 구조 단순화
  - 업데이트 게이트와 리셋 게이트만 사용
  - 셀 상태와 은닉 상태 통합
  - 파라미터 수 감소
- LSTM을 사용하며 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU 사용 필요 없음
- 데이터 양이 적을 때는 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 나은 경향을 보임
- 은닉상태 h는 입력이 들어올 때마다 계산되는 ‘동적인 상태’이기 때문에 계속 변하고, 가중치 W는 학습을 통해 얻은 ‘고정된 규칙’이기 때문에 추론 단계에서는 변하지 않음
3. 양방향 순환 신경망 : 시퀀스를 정방향과 역방향으로 처리하는 구조 -> 과거 미래 정보 모두 사용 가능
  -  품사 태깅，명명된 개체 인식，감성 분석과 같이 앞뒤 문맥을 확인해야 하는 경우에 유리
### 6.3 파이토치를 이용한 순환 신경망 구현(1)
- 현실의 시퀀스는 길이가 천차만별 -> 하지만 신경망은 모든 샘플의 길이가 같아야 함을 요구 (텐서로 묶기 위해)
  - sol ) 패딩을 통해 짧은 시퀀스 뒤에 의미 없는 토큰 0을 붙여 길이를 맞춤
  - 하지만 불필요한 계산이나 노이즈가 생긴다는 점에서 문제
  - sol ) 패킹을 통해 실제 길이 정보를 함께 제공하여 pad부분을 무시하게 함
  - 결론 : 길이를 맞춰야 하기 때문에 일단 패딩 필요 + 의미 있는 부분만 연산할 수 있게 패킹
### 6.4 파이토치를 이용한 순환 신경망 구현(2)
- LSTM 모델 : 시간적 의존성을 가지는 시계열 데이터, 특히 주가 데이터를 효과적으로 처리할 수 있는 강력한 신경망 모델
- StockPredictionModel 구성 요소
- StockPredictionModel 클래스는 다음과 같은 주요 구성 요소로 이루어져 있음
1. LSTM 계층
  - 과거 N일간의 주가 시퀀스 데이터를 입력으로 받아 시간적 패턴을 학습함
  - num_layers 매개변수를 통해 LSTM 계층의 깊이(층 수)를 조절할 수 있음
  - 여러 LSTM 계층 사이에는 Dropout이 적용되어 과적합을 방지함
2. Dropout
  - LSTM 계층과 완전 연결 계층 사이에 적용됨
  - 학습 과정에서 일부 뉴런을 무작위로 비활성화하여 모델의 일반화 성능을 향상시킴
3. 완전 연결 계층 (Fully Connected Layer)
  - LSTM의 마지막 시간 단계 출력을 입력으로 받음
  - 최종적으로 다음 날의 종가(예측값)를 출력함
입력 데이터 구성
  - 모델은 N일 동안의 주가 데이터를 하나의 시퀀스로 입력받음
  - 각 시간 단계의 입력은 다음과 같은 특성(feature)들로 구성됨
  - 시가(Open)
  - 고가(High)
  - 저가(Low)
  - 종가(Close)
  - 거래량(Volume) 등
  - 따라서 모델의 입력 크기(input_size)는 각 시간 단계에서 사용되는 특성의 개수에 따라 결정됨
- 주가의 전반적인 방향성은 예측할 수 있지만 급격한 변화는 예측이 어려움
- LSTM 모델의 시계열 예측 결과 성능을 분석하면 패턴 학습 능력이 향상되고 오차 변동성이 줄어들게 됨
### 6.5 자연어 처리 응용
1. 컴퓨터가 처리할 수 있도록 텍스트를 토큰화 해야함 -> 텍스트를 인덱스로 변환
2. 문장마다 길이가 다르기 때문에, 여러 문장을 한 번에 모델에 넣기 위해 길이를 맞추는 패딩과 묶어서 처리하는 방식(배치)을 사용
3. 어텐션 메커니즘의 사용 : 모든 단어를 똑같이 보지 않고, 의미를 결정하는 단어에 더 큰 가중치를 두고 판단할 수 있어서 감정 분류 성능이 좋아짐
4. 임베딩 : 이산적인 객체(단어, 토큰)를 의미 정보를 보존한 채 연속적인 실수 벡터로 변환하는 함수 -> 감성분석에서 비슷한 감정을 가진 단어를 비슷하게 표현하기 위해서 사용
5. 양방향 LSTM

```
python
self.lstm = nn.LSTM(
            input_size=embed_dim,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if n_layers > 1 else 0
        )
```
6. 코드 리뷰
- 시퀀스 전체 정보 반영: BiLSTM이 양방향 정보를 모두 제공 → 문맥을 더 잘 반영
- 중요 단어 강조: Attention이 중요한 단어/토큰에 높은 가중치 → 최종 분류 성능 향상
- 파라미터 효율적: Linear 레이어 하나로 바로 분류 가능, 마지막 hidden만 쓰므로 계산량이 적음
### 6.6 순환 신경망 하이퍼파라미터 튜닝
1. 베이지안 최적화 : 함수의 최대값이나 최소값을 찾는 방법
  - 머신러닝 모델의 정확도를 최대화하고 싶음
  - 학습률, 배치 크기, 은닉층 크기 같은 하이퍼파라미터 조합이 너무 많음
  - 모든 조합을 실험하는 것은 비용(시간, 연산) 너무 큼
  - 베이지안 최적화는 이전 실험 결과를 기반으로 다음에 시도할 값을 똑똑하게 선택
  - 후보 점 몇 개 선택 - 성능 확인 - 성능 기반으로 함수 형태 추정 - 다음 후보 점 선택 - 반복 - 최적값 발견
3. 성능 기반 조정 - 모델이 더 이상 학습되지 않을 때 LR을 줄여 최적화 + 과적합 방지
4. if is_improvement → 성능이 좋아졌으면 best 갱신, patience 초기화
5. else → 성능이 개선되지 않았으면 patience 증가 → patience 초과 시 LR 감소
### 6.7 순환 신경망의 실전 활용과 최적화
1. LSTM 오토인코더 : 시퀀스를 압축(encoding) → 복원(decoding) 하는 신경망
- 예시 : 영어 -> 프랑스어 번역
- Encoder: 입력 시퀀스를 압축해 hidden state를 생성 -> ex) 인코더는 영어 문장을 읽고 → 문장 전체 의미를 하나의 벡터(hidden state)로 요약
- 문장 전체를 읽고 마지막에 한 문장으로 요약한 상태 → 그 벡터가 hidden state
- Decoder: hidden state를 바탕으로 원래 시퀀스를 재구성 -> 디코더는 인코더의 hidden vector를 받아서 한 단어씩 프랑스어 생성
- Output Layer: 디코더 출력 후 최종 값 계산
2. 인공 시계열 데이터 생성 함수

```
python
def generate_time_series(n_samples=1000, anomaly_positions=[200, 400, 600, 800]):
    # 1) 기본 시계열 생성: 사인파 + 노이즈
    t = np.linspace(0, 10, n_samples)
    series = 0.8 * np.sin(t) + 0.2 * np.sin(5 * t) + 0.1 * np.random.randn(n_samples)

    # 2) 트렌드 추가 (선형 증가)
    trend = 0.005 * np.arange(n_samples)
    series += trend

    # 3) 이상치 추가
    for pos in anomaly_positions:
        if pos < n_samples:
            # 갑작스러운 스파이크 추가
            series[pos] += 1.5 * np.random.rand() + 0.5

            # 또는 갑작스러운 드롭 추가
            if np.random.rand() > 0.5 and pos + 1 < n_samples:
                series[pos + 1] -= 1.2 * np.random.rand() + 0.3

    return series
```
- Gaussian 노이즈 추가 -> 정규분포를 따르는 무작위 값을 추가하여 현실 세계와 비슷해짐
- 시계열이 점점 증가하는 선형 트렌드 추가
- anomaly_positions에 지정된 위치에 갑작스러운 스파이크 또는 드롭 추가

3. 입력 시퀀스와 재구성 시퀀스 차이 최소화
- 인코더 → 시퀀스 압축(latent vector)
- 디코더 → 압축된 벡터로 시퀀스 재구성
- Linear layer → feature 차원 복원
