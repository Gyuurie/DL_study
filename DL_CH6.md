## 6. 순환 신경망(RNN)
### 6.1 순환 신경망의 기초
- 시계열 데이터 : 시간 흐름에 따라 순차적으로 수집된 데이터 -> 관측값이 시간적 의존성을 가짐
- RNN은 기본적으로 과거의 정보를 기억하면서 새로운 정보를 처리
- CNN이 이미지 데이터의 공간적 특징을 추출하여 학습한다면 RNN은 시계열 데이터의 시간적 특징을 추출하여 학습
- Feed Forward Neural Network : 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향함
- RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 지님
- 은닉 상태 : 순환 신경망의 핵심으로 네트워크 메모리 역할을 함

$$
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b)
$$

  - $W_x$: 입력 가중치  
  - $W_h$: 이전 은닉 상태 가중치  
  - $b$: 편향 (bias)
- 순환 신경망의 순전파와 역전파
  - 순전파 : 이전에 얻은 기억을 다음으로 넘김 ex) 나는 -> 밥을 -> 먹었다.
  - 역전파 : 틀린 이유를 과거까지 거슬러 올라가서 찾는 과정 = 출력 → 시간을 거슬러 → 가중치 수정
  - 과거로 갈수록 신호가 점점 약해짐 -> 기울기가 점점 0으로 -> 오래된 정보가 거의 학습이 안 됨
  - 반면에 가중치가 1보다 큰 경우에는 가중치 폭발이 일어남
  - `clip_grad_norm_`을 통해 이미 계산된 기울기를 잘라낼 수 있음
  ### 6.2 고급 순환 신경망 아키텍처
1. LSTM : 전통적인 RNN의 단점을 보완한 장단기 메모리
  - 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함
  - 입력 게이트 : 현재 정보를 기억하기 위한 게이트
  - 망각 게이트 : 셀 상태에서 어떤 정보를 버릴지 결정
  - 셀 상태 : 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함 -> 이 값을 현재 시점 t의 셀 상태라고 하며, 이 값은 다음 t+1 시점의 LSTM 셀로 넘겨지게 됨
  - 출력 게이트 : 셀 상태에서 어떤 정보를 출력할지 결정 -> 출력 게이트는 현재 시점 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값
  - 셀 상태가 하이퍼볼릭탄젠트 함수를 지나 -1~1 사이의 값이 되고 해당 값은 출력 게이트의 값과 연산되며 값이 걸러짐 -> 은닉 상태
2. GRU : LSTM의 간소화 버전 -> 장기 의존성 학습 능력을 유지하면서도 구조 단순화
  - 업데이트 게이트와 리셋 게이트만 사용
  - 셀 상태와 은닉 상태 통합
  - 파라미터 수 감소
- LSTM을 사용하며 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU 사용 필요 없음
- 데이터 양이 적을 때는 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 나은 경향을 보임
- 
3. 양방향 순환 신경망 : 시퀀스를 정방향과 역방향으로 처리하는 구조 -> 과거 미래 정보 모두 사용 가능
  -  품사 태깅，명명된 개체 인식，감성 분석과 같이 앞뒤 문맥을 확인해야 하는 경우에 유리
### 6.3 파이토치를 이용한 순환 신경망 구현(1)
- 현실의 시퀀스는 길이가 천차만별 -> 하지만 신경망은 모든 샘플의 길이가 같아야 함을 요구 (텐서로 묶기 위해)
  - sol ) 패딩을 통해 짧은 시퀀스 뒤에 의미 없는 토큰 0을 붙여 길이를 맞춤
  - 하지만 불필요한 계산이나 노이즈가 생긴다는 점에서 문제
  - sol ) 패킹을 통해 실제 길이 정보를 함께 제공하여 pad부분을 무시하게 함
  - 결론 : 길이를 맞춰야 하기 때문에 일단 패딩 필요 + 의미 있는 부분만 연산할 수 있게 패킹
### 6.4 파이토치를 이용한 순환 신경망 구현(2)
- LSTM 모델 : 시간적 의존성을 가지는 시계열 데이터, 특히 주가 데이터를 효과적으로 처리할 수 있는 강력한 신경망 모델
- StockPredictionModel 구성 요소
- StockPredictionModel 클래스는 다음과 같은 주요 구성 요소로 이루어져 있음
1. LSTM 계층
  - 과거 N일간의 주가 시퀀스 데이터를 입력으로 받아 시간적 패턴을 학습함
  - num_layers 매개변수를 통해 LSTM 계층의 깊이(층 수)를 조절할 수 있음
  - 여러 LSTM 계층 사이에는 Dropout이 적용되어 과적합을 방지함
2. Dropout
  - LSTM 계층과 완전 연결 계층 사이에 적용됨
  - 학습 과정에서 일부 뉴런을 무작위로 비활성화하여 모델의 일반화 성능을 향상시킴
3. 완전 연결 계층 (Fully Connected Layer)
  - LSTM의 마지막 시간 단계 출력을 입력으로 받음
  - 최종적으로 다음 날의 종가(예측값)를 출력함
입력 데이터 구성
  - 모델은 N일 동안의 주가 데이터를 하나의 시퀀스로 입력받음
  - 각 시간 단계의 입력은 다음과 같은 특성(feature)들로 구성됨
  - 시가(Open)
  - 고가(High)
  - 저가(Low)
  - 종가(Close)
  - 거래량(Volume) 등
  - 따라서 모델의 입력 크기(input_size)는 각 시간 단계에서 사용되는 특성의 개수에 따라 결정됨
- 주가의 전반적인 방향성은 예측할 수 있지만 급격한 변화는 예측이 어려움
- LSTM 모델의 시계열 예측 결과 성능을 분석하면 패턴 학습 능력이 향상되고 오차 변동성이 줄어들게 됨
