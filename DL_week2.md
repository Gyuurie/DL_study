## 4. 딥러닝 모델 구축
### 4.1 파이토치를 이용한 모델 학습 과정 이해
1. 순전파
- 입력 데이터를 모델의 각 레이어를 거쳐 최종 예측 결과를 도출하는 것
- 입력 데이터는 모델의 연산 흐름을 그대로 따르며 예측값을 형성함
- 단계
  - 입력 처리 : 이미지, 텍스트, 시계열 -> 모델이 이해할 수 있는 형태로 변형(전처리)
  - 계층별 연산 : 선형 변환 후 비선형 활성화 함수를 적용하여 데이터 패턴 학습
  - 출력 생성 : 확률 분포, 연속적 수치값
- 선형 회귀 모델 : $y = Wx + b$
- 활성화 함수 : 선형 -> 비선형 -> 표현력 확장

| 함수      | 출력 범위  | 장점      | 단점         | 주 사용 위치  |
| ------- | ------ | ------- | ---------- | -------- |
| Sigmoid | 0 ~ 1  | 확률 해석   | 기울기 소실     | 출력층      |
| Tanh    | -1 ~ 1 | 0 중심    | 기울기 소실     | (과거) 은닉층 |
| ReLU    | 0 ~ ∞  | 빠르고 효율적 | Dying ReLU | 은닉층      |

- 기울기 소실 : 역전파 과정에서 기울기가 점점 작아져 앞쪽 층 가중치 업데이트가 이루어지지 않음 -> 앞쪽 레이어 학습 불가
- Dying ReLU : ReLU 뉴런이 항상 0만 출력하고 기울기도 0이 되어 살아나지 않음

2. 손실 계산
- 엔트로피 손실 : 각 클래스에 대해 예측한 확률 분포와 실제 레이블 간 차이를 측정하여 정답 클래스에 더 큰 확률을 주어 분류 문제에 사용
- MSE : $(예측값-실제값)^2$ 오차가 클 수록 큰 페널티 부여하여 회귀 문제에 사용
- MAE : $|예측값-실제값|$ MSE에 비해 이상치에 덜 민감

3. 역전파
- 모델의 오차를 기반으로 가중치에 대한 손실 함수 기울기를 계산하여 가중치를 업데이트
- 오차 계산 -> 어떤 파라미터가 얼마나 영향? -> 이를 바탕으로 파라미터 조정
- 역전파 계산 시 학습 단계마다 기울기 초기화 `optimizer.zero_grad()`

4. 파라미터 업데이트
- 옵티마이저 : 역전파 과정에서 계산된 손실 함수의 기울기를 기반으로 각 파라미터를 업데이트
