## 4. 딥러닝 모델 구축
### 4.1 파이토치를 이용한 모델 학습 과정 이해
1. 순전파
- 입력 데이터를 모델의 각 레이어를 거쳐 최종 예측 결과를 도출하는 것
- 입력 데이터는 모델의 연산 흐름을 그대로 따르며 예측값을 형성함
- 단계
  - 입력 처리 : 이미지, 텍스트, 시계열 -> 모델이 이해할 수 있는 형태로 변형(전처리)
  - 계층별 연산 : 선형 변환 후 비선형 활성화 함수를 적용하여 데이터 패턴 학습
  - 출력 생성 : 확률 분포, 연속적 수치값
- 선형 회귀 모델 : $y = Wx + b$
- 활성화 함수 : 선형 -> 비선형 -> 표현력 확장

| 함수      | 출력 범위  | 장점      | 단점         | 주 사용 위치  |
| ------- | ------ | ------- | ---------- | -------- |
| Sigmoid | 0 ~ 1  | 확률 해석   | 기울기 소실     | 출력층      |
| Tanh    | -1 ~ 1 | 0 중심    | 기울기 소실     | (과거) 은닉층 |
| ReLU    | 0 ~ ∞  | 빠르고 효율적 | Dying ReLU | 은닉층      |

- 기울기 소실 : 역전파 과정에서 기울기가 점점 작아져 앞쪽 층 가중치 업데이트가 이루어지지 않음 -> 앞쪽 레이어 학습 불가
- Dying ReLU : ReLU 뉴런이 항상 0만 출력하고 기울기도 0이 되어 살아나지 않음

2. 손실 계산
- 엔트로피 손실 : 각 클래스에 대해 예측한 확률 분포와 실제 레이블 간 차이를 측정하여 정답 클래스에 더 큰 확률을 주어 분류 문제에 사용
- MSE : $(예측값-실제값)^2$ 오차가 클 수록 큰 페널티 부여하여 회귀 문제에 사용
- MAE : $|예측값-실제값|$ MSE에 비해 이상치에 덜 민감

3. 역전파
- 모델의 오차를 기반으로 가중치에 대한 손실 함수 기울기를 계산하여 가중치를 업데이트
- 오차 계산 -> 어떤 파라미터가 얼마나 영향? -> 이를 바탕으로 파라미터 조정
- 역전파 계산 시 학습 단계마다 기울기 초기화 `optimizer.zero_grad()`

4. 파라미터 업데이트
- 옵티마이저 : 역전파 과정에서 계산된 손실 함수의 기울기를 기반으로 각 파라미터를 업데이트
- `optimizer.zero_grad()` 사용
- 반복 학습을 통해 손실 줄임

| 옵티마이저 | 학습률 방식 | 핵심 아이디어 | 수렴 속도 | 안정성 | 일반화 성능 | 주 사용 상황 |
|-----------|------------|---------------|-----------|--------|-------------|--------------|
| SGD | 고정 | 현재 기울기 방향으로 이동 | 느림 | 낮음 | 높음 | 최종 성능 중요, CNN |
| RMSProp | 적응형 | 기울기 제곱의 이동 평균 | 빠름 | 높음 | 중간 | 학습 불안정, RNN |
| Adam | 적응형 + 모멘텀 | Momentum + RMSProp 결합 | 매우 빠름 | 매우 높음 | 중간 | 기본 선택, 빠른 실험 |

- momentum : 이전까지의 기울기 방향을 기억하여 한 방향으로 수렴 -> 속도 증가
### 4.2 데이터 전처리와 DataLoader 사용
1. 데이터 준비 및 전처리
- 정규화 : 이미지의 픽셀값이나 기타 수치형 데이터를 일정 범위로 조정 -> 입력값 균등 -> 안정적이고 빠른 수렴
- 데이터 증강 : 원본 데이터를 회전, 이동, 크기 조절 등의 변환을 가하여 데이터를 생성 -> 오버피팅을 줄이는 기법 -> 다양한 상황에 일반화 가능
- 데이터 분할 : 전체 데이터 셋 => 학습 + 검증 + 테스트
2. 데이터로더 활용 및 배치 구성
- 배치 처리 : 배치 크기를 n개로 설정 -> n개의 샘플이 한 번에 모델에 입력 -> 계산 효율 극대화
- 데이터 셔플 : 학습 중 에포크마다 데이터 순서 무작위 처리 -> 순서 의존 방지
- 멀티프로세싱  : num workers 옵션을 통해 데이터를 여러 프로세스에서 동시에 불러올 수 있음
### 4.3 모델 성능 평가와 개선
1. 평가 지표 및 모델 검증

| 문제 유형 | 평가 지표          | 정의                          | 특징                               |
| ----- | -------------- | --------------------------- | -------------------------------- |
| 분류 문제 | 정확도 (Accuracy) | 모델이 예측한 클래스와 실제 정답이 일치하는 비율 | 직관적이고 이해하기 쉬움, 클래스 불균형 데이터에서는 한계 |
| 회귀 문제 | 평균 제곱 오차 (MSE) | 예측값과 실제값의 차이를 제곱한 후 평균한 값   | 큰 오차에 더 큰 페널티 부여, 이상치에 민감        |
| 회귀 문제 | 평균 절대 오차 (MAE) | 예측값과 실제값 차이의 절대값을 평균한 값     | 이상치에 덜 민감, 오차 해석이 직관적            |

- 뉴런 = 선형 결합 + 활성화 함수
- 드롭아웃 기법은 학습용 기법으로 일부 뉴런의 영향을 0으로 만듦 -> 오버피팅 막기
- 드롭아웃은 평가 시에 사용 X -> 랜덤성 제거, 일관성을 위해
- 배치 정규화 : 각 채널의 입력 평균 0, 분산 1에 가깝게 정규화 -> 입력 분포가 항상 비슷 -> 기울기 흐름 개선 및 학습 안정화
- `torch.no_grad()` : 기울기 계산을 막는 코드 -> 모델 평가 시에만 사용하는 코드 

| 항목                 | 학습      | 평가     |
| ------------------ | ------- | ------ |
| model.train / eval | train() | eval() |
| Dropout            | ON      | OFF    |
| BatchNorm          | 배치 통계   | 러닝 통계  |
| 기울기 계산             | O       | X      |
| no_grad            | X       | O      |
| 가중치 업데이트           | O       | X      |
| 목적                 | 성능 개선   | 성능 측정  |

2. 모델 개선 및 하이퍼파라미터 튜닝
- 정규화 기법
  - 드롭아웃 : 일부 뉴런을 끔
  - 가중치 감소 : 가중치 크기 제한 -> 과도한 복잡성 방지
- 데이터 증강 : 다양한 상황 학습
- 하이퍼파라미터 튜닝 : 학습률, 배치 크기, 에포크 등 을 바꿔가며 실험 -> 최적의 모델 성능 도출
- 최적의 하이퍼파라미터 찾기

| 방법                                   | 탐색 방식                           | 장점                             | 단점                        | 주 사용 상황                  |
| ------------------------------------ | ------------------------------- | ------------------------------ | ------------------------- | ------------------------ |
| **Grid Search (격자 탐색)**              | 모든 하이퍼파라미터 조합을 **전부 탐색**        | 구현 간단<br>최적 조합 보장 (탐색 범위 내)    | 계산 비용 매우 큼<br>차원 증가 시 비효율 | 파라미터 수 적을 때<br>범위가 좁을 때  |
| **Random Search (무작위 탐색)**           | 지정한 범위에서 **무작위로 샘플링**           | Grid보다 효율적<br>중요 파라미터에 더 많은 탐색 | 최적값 보장 X<br>결과 변동성 있음     | 파라미터 많을 때<br>시간 제한 있을 때  |
| **Bayesian Optimization (베이지안 최적화)** | 이전 결과를 바탕으로 **확률 모델로 다음 탐색 결정** | 탐색 효율 매우 높음<br>적은 실험으로 최적값 접근  | 구현 복잡<br>계산 오버헤드 존재       | 학습 비용이 비쌀 때<br>고성능 모델 튜닝 |

- Confusion Matrix(혼동 행렬) : 모델이 어떤 클래스를 어떤 클래스로 자주 착각하는지를 행렬 형태로 나타낸 것
